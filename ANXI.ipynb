{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-23T01:06:29.429445Z",
     "iopub.status.busy": "2025-01-23T01:06:29.429134Z",
     "iopub.status.idle": "2025-01-23T01:06:32.344804Z",
     "shell.execute_reply": "2025-01-23T01:06:32.343683Z",
     "shell.execute_reply.started": "2025-01-23T01:06:29.429415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"anxiety_attack_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:06:39.886385Z",
     "iopub.status.busy": "2025-01-23T01:06:39.885832Z",
     "iopub.status.idle": "2025-01-23T01:06:39.920210Z",
     "shell.execute_reply": "2025-01-23T01:06:39.918688Z",
     "shell.execute_reply.started": "2025-01-23T01:06:39.886342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:06:54.796217Z",
     "iopub.status.busy": "2025-01-23T01:06:54.795802Z",
     "iopub.status.idle": "2025-01-23T01:06:54.850133Z",
     "shell.execute_reply": "2025-01-23T01:06:54.848847Z",
     "shell.execute_reply.started": "2025-01-23T01:06:54.796188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:06:56.513422Z",
     "iopub.status.busy": "2025-01-23T01:06:56.513011Z",
     "iopub.status.idle": "2025-01-23T01:06:56.542702Z",
     "shell.execute_reply": "2025-01-23T01:06:56.541053Z",
     "shell.execute_reply.started": "2025-01-23T01:06:56.513392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:07:11.540913Z",
     "iopub.status.busy": "2025-01-23T01:07:11.540477Z",
     "iopub.status.idle": "2025-01-23T01:07:11.567705Z",
     "shell.execute_reply": "2025-01-23T01:07:11.566344Z",
     "shell.execute_reply.started": "2025-01-23T01:07:11.540880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.drop('ID', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:07:26.259311Z",
     "iopub.status.busy": "2025-01-23T01:07:26.258862Z",
     "iopub.status.idle": "2025-01-23T01:07:26.292412Z",
     "shell.execute_reply": "2025-01-23T01:07:26.291060Z",
     "shell.execute_reply.started": "2025-01-23T01:07:26.259279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "columns = ['Gender', 'Occupation', 'Smoking', \n",
    "           'Family History of Anxiety', 'Dizziness', \n",
    "           'Medication', 'Recent Major Life Event']\n",
    "\n",
    "mappings = {}\n",
    "\n",
    "for column in columns:\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    mappings[column] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# check mappings\n",
    "for column, mapping in mappings.items():\n",
    "    print(f\"Mapping for {column}: {mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:07:33.933328Z",
     "iopub.status.busy": "2025-01-23T01:07:33.932878Z",
     "iopub.status.idle": "2025-01-23T01:07:33.954892Z",
     "shell.execute_reply": "2025-01-23T01:07:33.953620Z",
     "shell.execute_reply.started": "2025-01-23T01:07:33.933298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## visualization & data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_column(df, column_name):\n",
    "    unique_values = df[column_name].value_counts().sort_index()\n",
    "    \n",
    "    # Dynamically adjust the figure width based on the number of x-axis labels\n",
    "    num_labels = len(unique_values)\n",
    "    fig_width = max(6, num_labels * 0.5)  # Base width is 6, add 0.5 units for each additional label\n",
    "    plt.figure(figsize=(fig_width, 6))  # Fixed height is 6\n",
    "    \n",
    "    unique_values.plot(kind='bar', edgecolor='black')\n",
    "    plt.title(f'Count of Each Unique {column_name}')\n",
    "    plt.xlabel(f'{column_name}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels\n",
    "    plt.tight_layout()  # Automatically adjust layout to avoid label clipping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:07:54.156160Z",
     "iopub.status.busy": "2025-01-23T01:07:54.155757Z",
     "iopub.status.idle": "2025-01-23T01:07:54.169657Z",
     "shell.execute_reply": "2025-01-23T01:07:54.168443Z",
     "shell.execute_reply.started": "2025-01-23T01:07:54.156127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unique_ages = df['Age'].value_counts().sort_index()\n",
    "unique_ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idk why these ages are this much 'discrete', cuz usually in large scale dataset we can see a 'pseudo-continuous' distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:07:58.689919Z",
     "iopub.status.busy": "2025-01-23T01:07:58.689549Z",
     "iopub.status.idle": "2025-01-23T01:07:59.272313Z",
     "shell.execute_reply": "2025-01-23T01:07:59.270736Z",
     "shell.execute_reply.started": "2025-01-23T01:07:58.689894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_column(df, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T01:08:14.173639Z",
     "iopub.status.busy": "2025-01-23T01:08:14.173241Z",
     "iopub.status.idle": "2025-01-23T01:08:14.376334Z",
     "shell.execute_reply": "2025-01-23T01:08:14.375098Z",
     "shell.execute_reply.started": "2025-01-23T01:08:14.173611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.hist(df[\"Age\"], bins=20, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is not what I call healthy data.\n",
    "\n",
    "I want it better formed. To do so, I try to classify them using KMeans. For reusing this, I write it a func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def auto_cluster(df, column_name, cluster_column_name, verbose=False):\n",
    "    # Set the number of clusters\n",
    "    k = 5\n",
    "\n",
    "    # Use the best k value for clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    df[cluster_column_name] = kmeans.fit_predict(df[[column_name]])\n",
    "\n",
    "    if verbose:\n",
    "        # Print the original column and the cluster column\n",
    "        print(df[[column_name, cluster_column_name]])\n",
    "        print('\\n')\n",
    "\n",
    "        # Print the descriptive statistics for each cluster\n",
    "        print(df.groupby(cluster_column_name)[column_name].describe())\n",
    "\n",
    "        # Visualize the clusters\n",
    "        visualize_column(df, cluster_column_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df = auto_cluster(df, 'Age', 'Class of Age', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets go vis them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_column(df, 'Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oops, got some non-bisexual.\n",
    "\n",
    "I want to check if non-bisexuals are more likely to have anxiety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_bisexuals = df[df['Gender'] == 2]\n",
    "\n",
    "visualize_column(non_bisexuals, 'Severity of Anxiety Attack (1-10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_bisexuals['Severity of Anxiety Attack (1-10)'].describe())\n",
    "print('\\n')\n",
    "print(df[df['Gender']==0]['Severity of Anxiety Attack (1-10)'].describe())  # Male\n",
    "print('\\n')\n",
    "print(df[df['Gender']==1]['Severity of Anxiety Attack (1-10)'].describe())  # Female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, non-bisexuals are more likely to have anxiety than bisexuals. Meanwhile, females are more likely to have anxiety than males. \n",
    "\n",
    "You can see it from the mean and std.\n",
    "\n",
    "Men just don't give a f*ck to shit things right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_column(df, 'Sleep Hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to much 'average'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_cluster(df, 'Sleep Hours', 'Class of Sleep Hours', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we may see, short duration sleep is class 0, followed by class 1 and 2 which are the medium and long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_column(df, 'Physical Activity (hrs/week)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we do the same trick on Physical Activity Duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_cluster(df, 'Physical Activity (hrs/week)', 'Class of Physical Activity', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Caffeine Intake (mg/day)\n",
    "df = auto_cluster(df, 'Caffeine Intake (mg/day)', 'Class of Caffeine Intake', verbose=True)\n",
    "\n",
    "# Cluster Alcohol Consumption (drinks/week)\n",
    "df = auto_cluster(df, 'Alcohol Consumption (drinks/week)', 'Class of Alcohol Consumption', verbose=True)\n",
    "\n",
    "# Cluster Therapy Sessions (per month)\n",
    "df = auto_cluster(df, 'Therapy Sessions (per month)', 'Class of Therapy Sessions', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# columns to drop:\n",
    "# you can select the columns whatever you like or dislike\n",
    "columns_to_drop = [\n",
    "    'Severity of Anxiety Attack (1-10)',\n",
    "    'Age',\n",
    "    'Sleep Hours',\n",
    "    'Physical Activity (hrs/week)',\n",
    "    'Caffeine Intake (mg/day)',\n",
    "    'Alcohol Consumption (drinks/week)',\n",
    "    'Therapy Sessions (per month)'\n",
    "]\n",
    "\n",
    "X = df.drop(columns=columns_to_drop, axis=1)\n",
    "y = df['Severity of Anxiety Attack (1-10)'] - 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from KAN import KANLinear\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, use_kan=False):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        \n",
    "        if use_kan:\n",
    "            from KAN import KANLinear as Linear\n",
    "        else:\n",
    "            from torch.nn import Linear\n",
    "        # 添加隐藏层\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(Linear(last_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_size = hidden_size\n",
    "\n",
    "        # 添加输出层\n",
    "        layers.append(nn.Linear(last_size, output_size))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# DIY hidden layers\n",
    "hidden_sizes = [16, 32, 48, 32, 16]\n",
    "\n",
    "mlp = MLP(input_size=X_train.shape[1], hidden_sizes=hidden_sizes, output_size=10, use_kan=True)\n",
    "\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "y_train_tensor = torch.LongTensor(y_train.values)\n",
    "X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "y_test_tensor = torch.LongTensor(y_test.values)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp = MLP(input_size=X_train.shape[1], hidden_sizes=hidden_sizes, output_size=10, use_kan=True)\n",
    "\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    mlp.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    train_log_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch', leave=False, mininterval=0.1, miniters=1, dynamic_ncols=True)\n",
    "\n",
    "    for batch_X, batch_y in train_log_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        batch_acc = (predicted == batch_y).float().mean().item()\n",
    "        batch_loss = loss.item()\n",
    "        train_log_bar.set_postfix(loss=batch_loss, acc=batch_acc)\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    mlp.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_log_bar = tqdm(test_loader, desc=f'Validation', unit='batch', leave=False, mininterval=0.1, miniters=1, dynamic_ncols=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_log_bar:\n",
    "            outputs = mlp(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "            batch_acc = (predicted == batch_y).float().mean().item()\n",
    "            batch_loss = loss.item()\n",
    "            val_log_bar.set_postfix(loss=batch_loss, acc=batch_acc)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, d_model=64, nhead=4, num_encoder_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        # 线性层将输入特征映射到 d_model 维度\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Transformer 编码器\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # 分类头\n",
    "        self.classifier = nn.Linear(d_model, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 输入投影\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Transformer 编码器需要输入形状为 (seq_len, batch_size, d_model)\n",
    "        x = x.unsqueeze(0)  # 添加序列长度维度\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # 取序列的第一个时间步的输出作为分类依据\n",
    "        x = x[0, :, :]\n",
    "        \n",
    "        # 分类\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TransformerClassifier(input_size=input_size, output_size=output_size)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 将数据转换为 PyTorch 张量\n",
    "X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "y_train_tensor = torch.LongTensor(y_train.values)\n",
    "X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "y_test_tensor = torch.LongTensor(y_test.values)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 初始化 TransformerClassifier\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 10\n",
    "model = TransformerClassifier(input_size=input_size, output_size=output_size)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    train_log_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch', leave=False, mininterval=0.1, miniters=1, dynamic_ncols=True)\n",
    "\n",
    "    for batch_X, batch_y in train_log_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        batch_acc = (predicted == batch_y).float().mean().item()\n",
    "        batch_loss = loss.item()\n",
    "        train_log_bar.set_postfix(loss=batch_loss, acc=batch_acc)\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_log_bar = tqdm(test_loader, desc=f'Validation', unit='batch', leave=False, mininterval=0.1, miniters=1, dynamic_ncols=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_log_bar:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "            batch_acc = (predicted == batch_y).float().mean().item()\n",
    "            batch_loss = loss.item()\n",
    "            val_log_bar.set_postfix(loss=batch_loss, acc=batch_acc)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    # 每10个epoch打印一次训练和验证结果\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6508490,
     "sourceId": 10514864,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
